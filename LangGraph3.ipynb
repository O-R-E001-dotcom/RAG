{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef03524a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_chroma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_chroma'"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"‚úÖ API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217feff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77377379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"working_with_llms/Agentic-RAG/psych-pdf\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"‚ö†Ô∏è File not found: {file_path}\")\n",
    "    print(\"Please update the file_path variable with your PDF file.\")\n",
    "    print(\"\\nFor this demo, we'll create sample documents instead...\")\n",
    "    \n",
    "    \n",
    "# Load the PDF\n",
    "loader = PyPDFDirectoryLoader(file_path)\n",
    "pages = []\n",
    "    \n",
    "# Load pages (async loading)\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "    \n",
    "print(f\"‚úÖ Loaded {len(pages)} pages from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16610dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter (Module 2 knowledge!)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Characters per chunk\n",
    "    chunk_overlap=100     # Overlap to preserve context\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"‚úÖ Created {len(doc_splits)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"{doc_splits[0].page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings (using OpenAI)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Embeddings model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chroma vector store\n",
    "chroma_path = \"./chroma_db_agentic_rag\"\n",
    "\n",
    "# Create vector store from documents\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"agentic_rag_docs\",\n",
    "    persist_directory=chroma_path,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "vectorstore.add_documents(documents=doc_splits)\n",
    "\n",
    "print(f\"‚úÖ Vector store created with {len(doc_splits)} chunks\")\n",
    "print(f\"   Persisted to: {chroma_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_documents(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search for relevant documents in the knowledge base.\n",
    "    \n",
    "    Use this tool when you need information from the document collection\n",
    "    to answer the user's question. Do NOT use this for:\n",
    "    - General knowledge questions\n",
    "    - Greetings or small talk\n",
    "    - \n",
    "    \n",
    "    Args:\n",
    "        query: The search query describing what information is needed\n",
    "        \n",
    "    Returns:\n",
    "        Relevant document excerpts that can help answer the question\n",
    "    \"\"\"\n",
    "    # Use MMR (Maximum Marginal Relevance) for diverse results\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\"k\": 5, \"fetch_k\": 10}\n",
    "    )\n",
    "    \n",
    "    # Retrieve documents\n",
    "    results = retriever.invoke(query)\n",
    "    \n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    # Format results\n",
    "    formatted = \"\\n\\n---\\n\\n\".join(\n",
    "        f\"Document {i+1}:\\n{doc.page_content}\"\n",
    "        for i, doc in enumerate(results)\n",
    "    )\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "print(\"‚úÖ Retrieval tool created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc291525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tool directly\n",
    "test_result = retrieve_documents.invoke({\"query\": \"What is DNA?\"})\n",
    "print(f\"Tool result (first 300 chars):\\n{test_result[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f760c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(content=\"\"\"You are a helpful assistant with access to a document retrieval tool.\n",
    "\n",
    "RETRIEVAL DECISION RULES:\n",
    "\n",
    "DO NOT retrieve for:\n",
    "- Greetings: \"Hello\", \"Hi\", \"How are you\"\n",
    "- Questions about your capabilities: \"What can you help with?\", \"What do you do?\"\n",
    "- General knowledge\n",
    "- Casual conversation: \"Thank you\", \"Goodbye\"\n",
    "\n",
    "DO retrieve for:\n",
    "- Questions asking for specific information that would be in documents\n",
    "- Requests for facts, definitions, or explanations about specialized topics\n",
    "- Any question where citing sources would improve the answer\n",
    "\n",
    "Rule of thumb: If the user is asking for information (not just chatting), retrieve first.\n",
    "\n",
    "When you retrieve documents, cite them in your answer. If documents don't contain the answer, say so.\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ System prompt configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da81dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind tool to LLM\n",
    "tools = [retrieve_documents]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def assistant(state: MessagesState) -> dict:\n",
    "    \"\"\"\n",
    "    Assistant node - decides whether to retrieve or answer directly.\n",
    "    \"\"\"\n",
    "    messages = [system_prompt] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"\n",
    "    Decide whether to call tools or finish.\n",
    "    \"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "print(\"‚úÖ Agent nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    should_continue,\n",
    "    {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "agent = builder.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ Agentic RAG system compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af2534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_agent(user_input: str, thread_id: str = \"default_session\"):\n",
    "    \"\"\"\n",
    "    Improved query function with clearer output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üë§ User: {user_input}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    result = agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]},\n",
    "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "\n",
    "    # Check what happened\n",
    "    used_retrieval = False\n",
    "    final_answer = None\n",
    "\n",
    "    for message in result[\"messages\"]:\n",
    "        if isinstance(message, AIMessage):\n",
    "            if message.tool_calls:\n",
    "                used_retrieval = True\n",
    "                print(f\"üîç Agent: [Calling retrieval tool...]\")\n",
    "            if message.content and not message.tool_calls:\n",
    "                final_answer = message.content\n",
    "\n",
    "    # Always print final answer\n",
    "    if final_answer:\n",
    "        print(f\"ü§ñ Agent: {final_answer}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No response generated after retrieval!\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nüìä Decision: {'USED RETRIEVAL' if used_retrieval else 'ANSWERED DIRECTLY'}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb580e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 1: Retrieval Needed (Technical definition)\n",
    "query_agent(\"What are the main use cases for WebSockets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13284cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 2: General Knowledge (No retrieval)\n",
    "query_agent(\"Hi, who are you and what can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51964903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 3: Retrieval Needed (Specific process)\n",
    "query_agent(\"Explain the WebSocket handshake process involved in connecting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 4: General Knowledge (Coding task)\n",
    "query_agent(\"Write a Python function to calculate the Fibonacci sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 5: Retrieval Needed (Comparison)\n",
    "query_agent(\"How does WebSocket performance compare to HTTP polling?\", thread_id=\"nonsense0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0161f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 6: General Knowledge (Simple Math)\n",
    "query_agent(\"What is 15% of 200?\", thread_id=\"nonsense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 7: Retrieval Needed (Specific protocol details)\n",
    "query_agent(\"What headers are sent in a WebSocket upgrade request?\", thread_id=\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e39980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
